<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <!--
  <script src="./resources/jsapi" type="text/javascript"></script>
  <script type="text/javascript" async>google.load("jquery", "1.3.2");</script>
 -->

<style type="text/css">
  @font-face {
   font-family: 'Avenir Book';
   src: url("./fonts/Avenir_Book.ttf"); /* File to be stored at your site */
   }

  body {
    font-family: "Avenir Book", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:14px;
    margin-left: auto;
    margin-right: auto;
    width: 1000px;
  }
  div {
    width: 800px;
    margin: auto;
  }

  figure {
    padding-right: 20px;
    padding-bottom: 40px;
    margin: auto;
    height: 160px;
    float: left;
  }
  figcaption {
    padding: 2px;
    text-align: center;
  }

  h1 {
    font-weight:300;
  }
  h2 {
    font-weight:300;
  }

  p {
    font-weight:300;
    line-height: 1.4;
  }

  code {
    font-size: 0.8rem;
    margin: 0 0.2rem;
    padding: 0.5rem 0.8rem;
    white-space: nowrap;
    background: #efefef;
    border: 1px solid #d3d3d3;
    color: #000000;
    border-radius: 3px;
  }

  pre > code {
    display: block;
    white-space: pre;
    line-height: 1.5;
    padding: 0;
    margin: 0;
  }

  pre.prettyprint > code {
    border: none;
  }


  .container {
        display: flex;
        align-items: center;
        justify-content: center
  }
  .image {
        flex-basis: 40%
  }
  .text {
        padding-left: 20px;
        padding-right: 20px;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 0px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;

  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    width: 800px;
  }
</style>

	<title>PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents</title>
</head>

<body>
	<br>
    <div>
        <!-- <div align="center"> -->
        <span style="font-size:32px; display:inline-block; text-align:center">PMC-CLIP: Contrastive Language-Image Pre-training <br>using Biomedical Documents</span><br><br><br>
    </div>
	<!-- <span style="font-size:36px" width="800px">PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents</span><br><br><br> -->
	<table align="center">
        <tbody><tr>
            <td align="center" width="120px">
                <center>
                <span style="font-size:16px"><a href="https://weixionglin.github.io/">Weixiong Lin</a><sup>1,</sup></span>
                </center>
            </td>
            <td align="center" width="120px">
                <center>
                <span style="font-size:16px">Ziheng Zhao<sup>1,</sup></span>
                </center>
            </td>
            <td align="center" width="160px">
                <center>
                <span style="font-size:16px"><a href="https://xiaoman-zhang.github.io/">Xiaoman Zhang</a><sup>1,2,</sup></span>
                </center>
            </td>
            <td align="center" width="130px">
                <center>
                <span style="font-size:16px"><a href="https://chaoyi-wu.github.io/">Chaoyi Wu</a><sup>1,2,</sup></span>
                </center>
            </td>
            <td align="center" width="100px">
                <center>
                <span style="font-size:16px"><a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a><sup>1,2,</sup></span>
                </center>
    		</td>
            <td align="center" width="150px">	    
                <center>
                <span style="font-size:16px"><a href="https://cmic.sjtu.edu.cn/wangyanfeng/">Yanfeng Wang</a><sup>1,2,</sup></span>
                </center>
    		</td>
            <td align="center" width="120px">
                <center>
                <span style="font-size:16px"><a href="https://weidixie.github.io/">Weidi Xie</a><sup>1,2,</sup></span>
                </center>
            </td>
        </tr></tbody>
    </table><br>
	
	<table align="center" width="700px">
        <tbody><tr>
            <td align="center" width="50px">
                <center>
                    <span style="font-size:16px"></span>
                </center>
            </td>
            <td align="center" width="300px">
                <center>
                    <span style="font-size:16px"><sup>1</sup>CMIC, Shanghai Jiao Tong University</span>
                </center>
            </td>
            <td align="center" width="300px">
                <center>
                    <span style="font-size:16px"><sup>2</sup>Shanghai AI Lab</span>
                </center>
            </td>
        </tr></tbody>
    </table>
	
	<table align="center" width="800px">
    <tbody><tr>
      <td align="center" width="200px">
      <center>
          <br>
          <span style="font-size:20px">Code
          <a href="https://github.com/WeixiongLin/PMC-CLIP/">[GitHub]</a>
          </span>
      </center>
      </td>

      <td align="center" width="200px">
      <center>
          <br>
          <span style="font-size:20px">
          Paper <a href="https://arxiv.org/abs/2303.07240"> [arXiv]</a>
          </span>
      </center>
      </td>

      <td align="center" width="200px">
        <center>
            <br>
            <span style="font-size:20px">
            Cite <a href="./cite.txt"> [BibTeX]</a>
            </span>
        </center>
    </td>
  
      <td align="center" width="400px">
        <center>
            <br>
            <span style="font-size:20px">
            Dataset <a href="https://huggingface.co/datasets/axiong/pmc-oa">[ðŸ¤—]</a>
            <a href="https://pan.baidu.com/s/1mD51oOYbIOqDJSeiPNaCCg">[Baidu]</a> <span style="font-size: 15px">(key: 3iqf)</span>
            </span>
        </center>
      </td>
  
    </tr></tbody>
  </table>
	
  <br><hr>
  <center><h2> Abstract </h2> </center>
  <div>
      <p style="text-align:justify; text-justify:inter-ideograph;"><left>
          Foundation models trained on large-scale dataset gain a recent surge in CV and NLP. In contrast, development in biomedical domain lags far behind due to data scarcity. To address this issue, we build and release PMC-OA, a biomedical dataset with 1.6M image-caption pairs collected from PubMedCentral's OpenAccess subset, which is 8 times larger than before. PMC-OA covers diverse modalities or diseases, with majority of the image-caption samples aligned at finer-grained level, i.e., subfigure and subcaption. While pretraining a CLIP-style model on PMC-OA, our model named PMC-CLIP achieves state-of-the-art results on various downstream tasks, including image-text retrieval on ROCO, MedMNIST image classification, Medical VQA, i.e. +8.1% R@10 on image-text retrieval, +3.9% accuracy on image classification.
      </left></p>
  </div>

  <br><hr>
  <center> <h2> Dataset Statistics Over the Pipeline </h2> </center>
  <div>
    <p style="text-align:justify; text-justify:inter-ideograph;"><left>
      Only 3.1% images of all are kept as biomedical figures.
      For figures with multiple pannels, we link subfigures with subcaptions to generate aligned samples.
      At last, 1.65M image-text pairs are left in PMC-OA
    </left></p>
    <p><img class="left"  src="./resources/data_stastics.png" width="800px"></p>
  </div>


  <br><hr>
  <center> <h2> Dataset Overview </h2> </center>
  <div>
    <p style="text-align:justify; text-justify:inter-ideograph;"><left>
      <i>First</i>, PMC-OA covers a wide range of diagnostic procedures, spanning from common (CT, MRI) to rare ones (mitotic figure), which is more diverse than before (Fig. (a)).
      <i>Second</i>, PMC-OA contains various diseases and findings, and is more up-to-date, covering new emergent diseases like COVID-19 (Fig. (b)).
      <i>Third</i>, The sex-ratio across ages are shown in Fig. (c), as we can see PMC-OA is approximately gender-balanced, with 54% males.
    </left></p>
  </div>
  <div>
    <figure>
      <img src="./resources/diagnostic.png" alt="diagnostic" style="height:100%">
      <figcaption>(a) Diagnostic procedure</figcaption>
    </figure>
    <figure>
      <img src="./resources/disease.png" alt="disease" style="height:100%">
      <figcaption>(b) Disease and findings</figcaption>
    </figure>
    <figure>
      <img src="./resources/ages.png" alt="disease" style="height:100%">
      <figcaption>(c) Patients' age & gender</figcaption>
    </figure>
  </div>

  <br><hr>
  <center> <h2> Pipeline </h2> </center>
  <div>
      <p style="text-align:justify; text-justify:inter-ideograph;"><left>
      Overview of the pipeline. 
      (Top.) Collect image-caption dataset(PMC-OA) from biomedical documents.
      (Bottom.) Pretrain PMC-CLIP on PMC-OA.
      </left></p>
      <p><img class="left"  src="./resources/pipeline.png" width="800px"></p>
  </div>

  <br><hr>
  <center><h2>Results</h2></center>
  <div>
    <p style="text-align:justify; text-justify:inter-ideograph;"><left>
        We evaluate PMC-CLIP on three downstream tasks: Image-text Retrieval, Medical Image Classification, Medical VQA.
    </left></p>
  </div>
  <div>
      <!-- =================== Retrieval  ======================== -->
      <p><b>R1: Image-text Retrieval </b> </p>
      <p><left>
          <!-- this is retrieval -->
          PMC-CLIP achieves a state-of-the-art result on image-text retrieval. On I2T Rank@10, PMC-CLIP outperforms previous state-of-the-art by 8.1%.
          Since our dataset does not contain data from ROCO, the reported results resembles zero-shot evaluation.
          Dark and light grey colors high-light the top and second best results on each metric.
      </left></p>
      <p><img class="center"  src="./resources/retrieval.png" width="800px"></p>

      <!-- =================== Image Classification  ======================== -->
      <p><b>R2: Medical Image Classification </b> </p>
      <p><left>
        PMC-CLIP achieves SOTA on MedMNIST. We only report three out of the twelve 2D-subset in MedMNIST here.
      </left></p>
      <p><img class="center"  src="./resources/classification.png" width="800px"></p>

      <!-- =================== VQA  ======================== -->
      <p><b>R3: Medical VQA </b> </p>	
      <p><left>
        VQA requires model to learn finer grain visual and language representations. PMC-CLIP surpass SOTA method M3AE in 5 out of 6 results.
      </left></p>
      <p><img class="center"  src="./resources/medvqa.png" width="800px"></p>
  </div>

  <br><hr>
  <center> <h2> Acknowledgements </h2> </center>
  <div>
      <p> 
          Based on a template by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a>.
      </p><br>
  </div><br>

</body>
</html>
